import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
from google.colab import files
from PIL import Image
import io

# === Load and Check Data ===
print("Loading data...")
cataract_images = np.load('/content/drive/MyDrive/Dataset/Cataract_Eye.npy')
normal_images = np.load('/content/drive/MyDrive/Dataset/normal_Eye.npy')
print(f"Loaded {cataract_images.shape[0]} cataract images.")
print(f"Loaded {normal_images.shape[0]} normal images.")

# Create labels: 1 = cataract, 0 = normal
cataract_labels = np.ones(cataract_images.shape[0])
normal_labels = np.zeros(normal_images.shape[0])

# Combine
images = np.concatenate((cataract_images, normal_images), axis=0)
labels = np.concatenate((cataract_labels, normal_labels), axis=0)
print(f"Total images combined: {images.shape[0]}")

# Check class distribution
unique, counts = np.unique(labels, return_counts=True)
print("Label distribution:", dict(zip(unique, counts)))

# === Preprocessing ===
print("Preprocessing images for ResNet50...")
images = preprocess_input(images)

# === Split ===
print("Splitting data into training and testing sets...")
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)
print(f"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}")

# === One-hot encode ===
print("Labels converted to one-hot encoding.")
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)

# === Model ===
print("Building the model...")
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(images.shape[1], images.shape[2], images.shape[3]))
base_model.trainable = False
print("ResNet50 base model loaded and frozen.")

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(2, activation='softmax')
])

print("Compiling the model...")
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#Summary
model.summary()

#Train
print("Starting model training...")
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)

#Plot accuracy and loss over epochs.
plt.figure(figsize=(12,4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

#Evaluate
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {acc:.4f}, Loss: {loss:.4f}")

# Upload & Predict ===
uploaded = files.upload()

for file_name in uploaded.keys():
    img = Image.open(io.BytesIO(uploaded[file_name])).convert('RGB')
    img_resized = img.resize((224, 224))
    img_array = image.img_to_array(img_resized)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)

    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)[0]

    # Mapping
    class_labels = {0: "normal eye", 1: "cataract eye"}
    result = class_labels[predicted_class]

    # Display
    print(f"Prediction probabilities: {prediction}")
    plt.imshow(img)
    plt.axis('off')
    plt.title(f"Prediction: {result}", fontsize=16)
    plt.show()
